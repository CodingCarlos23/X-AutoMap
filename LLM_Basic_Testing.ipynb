{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af38c5fb-9c3d-49dd-92d2-ff4877eba84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Using cached llama_cpp_python-0.3.9.tar.gz (67.9 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /home/codingcarlos/anaconda3/lib/python3.11/site-packages (from llama-cpp-python) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/codingcarlos/anaconda3/lib/python3.11/site-packages (from llama-cpp-python) (1.26.4)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /home/codingcarlos/anaconda3/lib/python3.11/site-packages (from llama-cpp-python) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codingcarlos/anaconda3/lib/python3.11/site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.3)\n",
      "Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.9-cp311-cp311-linux_x86_64.whl size=4149611 sha256=7b24d9cba2d00fec30a1c13001d6ff0b582e0981c7001961544ad6d805f9984e\n",
      "  Stored in directory: /home/codingcarlos/.cache/pip/wheels/9e/8f/bf/148c8eb7d69021eccd6eae6444f3accd48347587054ffd24e5\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: diskcache, llama-cpp-python\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install llama-cpp-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd216904-a125-450f-b4d1-0a37728db5b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 195 tensors from Phi-3-mini-4k-instruct-q4.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
      "llama_model_loader: - kv   1:                               general.name str              = Phi3\n",
      "llama_model_loader: - kv   2:                        phi3.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                      phi3.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   4:                   phi3.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv   5:                           phi3.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  phi3.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:               phi3.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   8:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv   9:                  phi3.rope.dimension_count u32              = 96\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 32000\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:   81 tensors\n",
      "llama_model_loader: - type q5_K:   32 tensors\n",
      "llama_model_loader: - type q6_K:   17 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 2.23 GiB (5.01 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control-looking token:  32007 '<|end|>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
      "load: control-looking token:  32000 '<|endoftext|>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special tokens cache size = 67\n",
      "load: token to piece cache size = 0.1690 MB\n",
      "print_info: arch             = phi3\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 4096\n",
      "print_info: n_embd           = 3072\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 32\n",
      "print_info: n_rot            = 96\n",
      "print_info: n_swa            = 2047\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 96\n",
      "print_info: n_embd_head_v    = 96\n",
      "print_info: n_gqa            = 1\n",
      "print_info: n_embd_k_gqa     = 3072\n",
      "print_info: n_embd_v_gqa     = 3072\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 8192\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 4096\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 3B\n",
      "print_info: model params     = 3.82 B\n",
      "print_info: general.name     = Phi3\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32064\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 32000 '<|endoftext|>'\n",
      "print_info: EOT token        = 32007 '<|end|>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: PAD token        = 32000 '<|endoftext|>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 32000 '<|endoftext|>'\n",
      "print_info: EOG token        = 32007 '<|end|>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 114 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:  CPU_AARCH64 model buffer size =  1242.00 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  2281.66 MiB\n",
      "repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.24.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.27.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.28.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.28.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.29.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.30.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.31.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.ffn_up.weight with q4_K_8x8\n",
      "...........................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 2048\n",
      "llama_context: n_ctx_per_seq = 2048\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 2048 (padded)\n",
      "llama_kv_cache_unified: kv_size = 2048, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1, padding = 32\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   768.00 MiB\n",
      "llama_kv_cache_unified: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context:        CPU compute buffer size =   168.01 MiB\n",
      "llama_context: graph nodes  = 1350\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\\n' + message['content'] + '<|end|>' + '\\n' + '<|assistant|>' + '\\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\\n'}}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '32000', 'tokenizer.ggml.eos_token_id': '32000', 'tokenizer.ggml.bos_token_id': '1', 'general.architecture': 'phi3', 'phi3.context_length': '4096', 'phi3.attention.head_count_kv': '32', 'general.name': 'Phi3', 'tokenizer.ggml.pre': 'default', 'phi3.embedding_length': '3072', 'tokenizer.ggml.unknown_token_id': '0', 'phi3.feed_forward_length': '8192', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.block_count': '32', 'phi3.attention.head_count': '32', 'phi3.rope.dimension_count': '96', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\n",
      "' + message['content'] + '<|end|>' + '\n",
      "' + '<|assistant|>' + '\n",
      "'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\n",
      "'}}{% endif %}{% endfor %}\n",
      "Using chat eos_token: <|endoftext|>\n",
      "Using chat bos_token: <s>\n",
      "llama_perf_context_print:        load time =     840.51 ms\n",
      "llama_perf_context_print: prompt eval time =     840.40 ms /    12 tokens (   70.03 ms per token,    14.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =   15579.66 ms /    99 runs   (  157.37 ms per token,     6.35 tokens per second)\n",
      "llama_perf_context_print:       total time =   16476.02 ms /   111 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<|assistant|> A black hole is like a giant cosmic vacuum cleaner in space. It's a place where gravity is so strong that nothing, not even light, can escape from it. This happens because a black hole is made when a very big star runs out of fuel and collapses under its own gravity. It's called a \"black\" hole because it doesn't let any light out, so it's invisible to us!\n",
      "\n",
      "<|assistant|> In simpler terms,\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(model_path=\"Phi-3-mini-4k-instruct-q4.gguf\", n_ctx=2048)\n",
    "\n",
    "prompt = \"Explain what a black hole is in simple terms.\"\n",
    "\n",
    "response = llm(prompt, max_tokens=100)\n",
    "print(response[\"choices\"][0][\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bb4931-cffd-44ee-84e1-68e47e44dde2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 195 tensors from Phi-3-mini-4k-instruct-q4.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
      "llama_model_loader: - kv   1:                               general.name str              = Phi3\n",
      "llama_model_loader: - kv   2:                        phi3.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                      phi3.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   4:                   phi3.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv   5:                           phi3.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  phi3.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:               phi3.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   8:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv   9:                  phi3.rope.dimension_count u32              = 96\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 32000\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:   81 tensors\n",
      "llama_model_loader: - type q5_K:   32 tensors\n",
      "llama_model_loader: - type q6_K:   17 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 2.23 GiB (5.01 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control-looking token:  32007 '<|end|>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
      "load: control-looking token:  32000 '<|endoftext|>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special tokens cache size = 67\n",
      "load: token to piece cache size = 0.1690 MB\n",
      "print_info: arch             = phi3\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 4096\n",
      "print_info: n_embd           = 3072\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 32\n",
      "print_info: n_rot            = 96\n",
      "print_info: n_swa            = 2047\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 96\n",
      "print_info: n_embd_head_v    = 96\n",
      "print_info: n_gqa            = 1\n",
      "print_info: n_embd_k_gqa     = 3072\n",
      "print_info: n_embd_v_gqa     = 3072\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 8192\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 4096\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 3B\n",
      "print_info: model params     = 3.82 B\n",
      "print_info: general.name     = Phi3\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32064\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 32000 '<|endoftext|>'\n",
      "print_info: EOT token        = 32007 '<|end|>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: PAD token        = 32000 '<|endoftext|>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 32000 '<|endoftext|>'\n",
      "print_info: EOG token        = 32007 '<|end|>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 114 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:  CPU_AARCH64 model buffer size =  1242.00 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  2281.66 MiB\n",
      "repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.24.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.27.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.28.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.28.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.29.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.30.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.31.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.ffn_up.weight with q4_K_8x8\n",
      "...........................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 2048\n",
      "llama_context: n_ctx_per_seq = 2048\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 2048 (padded)\n",
      "llama_kv_cache_unified: kv_size = 2048, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1, padding = 32\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   768.00 MiB\n",
      "llama_kv_cache_unified: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context:        CPU compute buffer size =   168.01 MiB\n",
      "llama_context: graph nodes  = 1350\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\\n' + message['content'] + '<|end|>' + '\\n' + '<|assistant|>' + '\\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\\n'}}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '32000', 'tokenizer.ggml.eos_token_id': '32000', 'tokenizer.ggml.bos_token_id': '1', 'general.architecture': 'phi3', 'phi3.context_length': '4096', 'phi3.attention.head_count_kv': '32', 'general.name': 'Phi3', 'tokenizer.ggml.pre': 'default', 'phi3.embedding_length': '3072', 'tokenizer.ggml.unknown_token_id': '0', 'phi3.feed_forward_length': '8192', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.block_count': '32', 'phi3.attention.head_count': '32', 'phi3.rope.dimension_count': '96', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\n",
      "' + message['content'] + '<|end|>' + '\n",
      "' + '<|assistant|>' + '\n",
      "'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\n",
      "'}}{% endif %}{% endfor %}\n",
      "Using chat eos_token: <|endoftext|>\n",
      "Using chat bos_token: <s>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Hi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     467.27 ms\n",
      "llama_perf_context_print: prompt eval time =     467.10 ms /     8 tokens (   58.39 ms per token,    17.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =   28176.73 ms /   199 runs   (  141.59 ms per token,     7.06 tokens per second)\n",
      "llama_perf_context_print:       total time =   28774.65 ms /   207 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Hi there! How can I help you today?\n",
      "\n",
      "<|assistant|> Hello! I'm an AI assistant, and I'm here to assist you with any questions or concerns you may have. Feel free to ask me anything!\n",
      "\n",
      "<|assistant|> Hello! I'm an AI developed by Microsoft, and I'm here to help you with any information or tasks you need assistance with. What can I do for you today?\n",
      "\n",
      "<|assistant|> Greetings! I'm an artificial intelligence designed to provide information and support. How may I be of service to you today?\n",
      "\n",
      "<|assistant|> Hey there! I'm an AI assistant and I'm here to help you with any questions or concerns you might have. What can I assist you with today?\n",
      "\n",
      "<|assistant|> Good day! I'm an AI developed to assist you with a variety of tasks. How may I assist you today?\n",
      "\n",
      "<|assistant|> To get started, I'm an AI assistant\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  talk to me about a star in the sky any star you decide\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 207 prefix-match hit, remaining 21 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     467.27 ms\n",
      "llama_perf_context_print: prompt eval time =    1122.18 ms /    21 tokens (   53.44 ms per token,    18.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =   27628.32 ms /   199 runs   (  138.84 ms per token,     7.20 tokens per second)\n",
      "llama_perf_context_print:       total time =   28877.28 ms /   220 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Of course! One fascinating star in the night sky is Vega.\n",
      "\n",
      "Vega is the fifth-brightest star in the night sky and is part of the constellation Lyra, the lyre. It is a relatively close star, at just 25 light-years away from Earth. Vega is classified as a main-sequence star of spectral type A0V, which means it is a hot, blue-white star that is fusing hydrogen into helium in its core.\n",
      "\n",
      "Here are some interesting facts about Vega:\n",
      "\n",
      "1. Vega is the fifth-brightest star visible from Earth and is often used as a calibration reference for determining stellar distances.\n",
      "2. It is one of the most luminous stars in our solar neighborhood, with a total apparent magnitude of 0.03.\n",
      "3. Vega has a mass about twice that of the Sun and is estimated\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  give me a long story about food\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 427 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =     467.27 ms\n",
      "llama_perf_context_print: prompt eval time =     941.08 ms /    15 tokens (   62.74 ms per token,    15.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =   30878.36 ms /   199 runs   (  155.17 ms per token,     6.44 tokens per second)\n",
      "llama_perf_context_print:       total time =   31957.97 ms /   214 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Once upon a time, in a bustling little village named Culinaris, there lived a humble yet ambitious young girl named Amara. Amara had a deep passion for food and dreamed of opening a magical restaurant that would serve the most extraordinary and delectable dishes. However, she knew that to achieve her dream, she needed to embark on a grand culinary adventure.\n",
      "\n",
      "Amara's quest began with a journey across the vast seas, where she encountered a variety of exotic ingredients and unique cooking techniques. One day, while exploring the islands of the Caribbean Sea, she met an old fisherman named Santiago, who taught her how to catch the most succulent and fresh fish using ancient methods passed down by his ancestors.\n",
      "\n",
      "With these newfound skills, Amara sailed further north, where she visited the snow-capped mountains of the Alps. There, she\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Load the model (adjust path and n_ctx as needed)\n",
    "llm = Llama(model_path=\"Phi-3-mini-4k-instruct-q4.gguf\", n_ctx=2048)\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "def build_prompt(history):\n",
    "    prompt = \"\"\n",
    "    for turn in history:\n",
    "        if turn[\"role\"] == \"user\":\n",
    "            prompt += f\"User: {turn['content']}\\n\"\n",
    "        else:\n",
    "            prompt += f\"Assistant: {turn['content']}\\n\"\n",
    "    prompt += \"Assistant:\"\n",
    "    return prompt\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in [\"bye\", \"exit\", \"quit\"]:\n",
    "            print(\"Assistant: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        prompt = build_prompt(chat_history)\n",
    "\n",
    "        response = llm(prompt, max_tokens=200, stop=[\"User:\", \"Assistant:\"])\n",
    "        reply = response[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "        print(\"Assistant:\", reply)\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": reply})\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n[Chat ended]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62883789-a154-41c1-a35c-2af09ded1580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the augmented blob data from disk\n",
    "with open(\"data/precomputed_blobs_with_real_info.pkl\", \"rb\") as f:\n",
    "    loaded_blobs = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "959bfd18-6dd7-4d64-bfa3-b00e7c7b3dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(loaded_blobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f5958be-b33c-417a-b13d-55936c70975a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color: red\n",
      "Threshold/Area: (0, 10)\n",
      "First blob contents:\n",
      "\n",
      "center: (512, 947)\n",
      "\n",
      "radius: 9\n",
      "\n",
      "color: red\n",
      "\n",
      "file: mosaic_200_Ca_merged.tiff\n",
      "\n",
      "max_intensity: 0.002844785340130329\n",
      "\n",
      "mean_intensity: 0.00025642229593358934\n",
      "\n",
      "mean_dilation: 1.5432098765432098\n",
      "\n",
      "box_x: 503\n",
      "\n",
      "box_y: 938\n",
      "\n",
      "box_size: 18\n",
      "\n",
      "real_center: (1034.0, 2861.0)\n",
      "\n",
      "real_box_size: (36.0, 54.0)\n",
      "\n",
      "real_box_area: 1944.0\n",
      "\n",
      "tooltip_html: <b>Red Element</b><br><i>mosaic_200_Ca_merged.tiff</i><br>Center: (512, 947)<br>Top-left: (503, 938)<br>Box size: 18 x 18 px<br>Box area: 324 px<br><br>Real Center location(m): (1034.00 m, 2861.00 m)<br>Real box size(m): (36.00 m  54.00 m)<br>Real box area(m): 1944.00 m<br><br>Max intensity: 0.003<br>Mean intensity: 0.000<br>Mean dilation intensity: 1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print all keys and values for the first blob in the data\n",
    "for color, blobs_by_key in loaded_blobs.items():\n",
    "    for key, blobs in blobs_by_key.items():\n",
    "        if blobs:  # Make sure it's not empty\n",
    "            first_blob = blobs[0]\n",
    "            print(f\"Color: {color}\")\n",
    "            print(f\"Threshold/Area: {key}\")\n",
    "            print(\"First blob contents:\\n\")\n",
    "\n",
    "            for k, v in first_blob.items():\n",
    "                print(f\"{k}: {v}\\n\")\n",
    "\n",
    "            # Exit after printing the first blob\n",
    "            break\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09a46970-208f-46b0-889b-34c2fdd06268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Fields for LLM Input:\n",
      "File: mosaic_200_Ca_merged.tiff\n",
      "Real Center (m): (1034.0, 2861.0)\n",
      "Real Box Size (m): (36.0, 54.0)\n",
      "Real Box Area (m): 1944.00\n",
      "Mean Intensity: 0.000256\n",
      "Max Intensity: 0.002845\n",
      "Mean Dilation: 1.543\n"
     ]
    }
   ],
   "source": [
    "# Print selected fields from the first blob\n",
    "for color, blobs_by_key in loaded_blobs.items():\n",
    "    for key, blobs in blobs_by_key.items():\n",
    "        if blobs:\n",
    "            blob = blobs[0]\n",
    "            print(\"Selected Fields for LLM Input:\")\n",
    "            print(f\"File: {blob['file']}\")\n",
    "            print(f\"Real Center (m): {blob['real_center']}\")\n",
    "            print(f\"Real Box Size (m): {blob['real_box_size']}\")\n",
    "            print(f\"Real Box Area (m): {blob['real_box_area']:.2f}\")\n",
    "            print(f\"Mean Intensity: {blob['mean_intensity']:.6f}\")\n",
    "            print(f\"Max Intensity: {blob['max_intensity']:.6f}\")\n",
    "            print(f\"Mean Dilation: {blob['mean_dilation']:.3f}\")\n",
    "            break\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd1f1021-9b1b-4108-9f08-f461dd304dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CSV file saved: data/blobs_llm_ready.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Ensure the folder exists\n",
    "import os\n",
    "os.makedirs(\"data_LLM\", exist_ok=True)\n",
    "\n",
    "# Path to save\n",
    "csv_path = \"data_LLM/blobs_llm_ready.csv\"\n",
    "\n",
    "# Open and write CSV\n",
    "with open(csv_path, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "\n",
    "    # Header row\n",
    "    writer.writerow([\n",
    "        \"file\",\n",
    "        \"real_center_x\", \"real_center_y\",\n",
    "        \"real_box_width\", \"real_box_height\",\n",
    "        \"real_box_area\",\n",
    "        \"mean_intensity\",\n",
    "        \"max_intensity\",\n",
    "        \"mean_dilation\"\n",
    "    ])\n",
    "\n",
    "    # Loop through all blobs in all colors and keys\n",
    "    for color, blobs_by_key in loaded_blobs.items():\n",
    "        for key, blobs in blobs_by_key.items():\n",
    "            for blob in blobs:\n",
    "                center_x, center_y = blob[\"real_center\"]\n",
    "                box_w, box_h = blob[\"real_box_size\"]\n",
    "\n",
    "                writer.writerow([\n",
    "                    blob[\"file\"],\n",
    "                    center_x, center_y,\n",
    "                    box_w, box_h,\n",
    "                    blob[\"real_box_area\"],\n",
    "                    blob[\"mean_intensity\"],\n",
    "                    blob[\"max_intensity\"],\n",
    "                    blob[\"mean_dilation\"]\n",
    "                ])\n",
    "\n",
    "print(f\" CSV file saved: {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98fe86f0-641f-44e5-acc0-663aa13e42ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>real_center_x</th>\n",
       "      <th>real_center_y</th>\n",
       "      <th>real_box_width</th>\n",
       "      <th>real_box_height</th>\n",
       "      <th>real_box_area</th>\n",
       "      <th>mean_intensity</th>\n",
       "      <th>max_intensity</th>\n",
       "      <th>mean_dilation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mosaic_200_Ca_merged.tiff</td>\n",
       "      <td>1034.0</td>\n",
       "      <td>2861.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1944.0</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.002845</td>\n",
       "      <td>1.543210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mosaic_200_Ca_merged.tiff</td>\n",
       "      <td>284.0</td>\n",
       "      <td>3119.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>864.0</td>\n",
       "      <td>0.002696</td>\n",
       "      <td>0.010303</td>\n",
       "      <td>7.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mosaic_200_Ca_merged.tiff</td>\n",
       "      <td>136.0</td>\n",
       "      <td>3116.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1176.0</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.013175</td>\n",
       "      <td>10.454082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mosaic_200_Ca_merged.tiff</td>\n",
       "      <td>796.0</td>\n",
       "      <td>3095.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.012170</td>\n",
       "      <td>7.435000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mosaic_200_Ca_merged.tiff</td>\n",
       "      <td>64.0</td>\n",
       "      <td>3107.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>0.002776</td>\n",
       "      <td>0.018371</td>\n",
       "      <td>12.162500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295278</th>\n",
       "      <td>mosaic_200_S_merged.tiff</td>\n",
       "      <td>972.0</td>\n",
       "      <td>1715.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>864.0</td>\n",
       "      <td>0.013238</td>\n",
       "      <td>0.033258</td>\n",
       "      <td>255.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295279</th>\n",
       "      <td>mosaic_200_S_merged.tiff</td>\n",
       "      <td>972.0</td>\n",
       "      <td>1715.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>864.0</td>\n",
       "      <td>0.013238</td>\n",
       "      <td>0.033258</td>\n",
       "      <td>255.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295280</th>\n",
       "      <td>mosaic_200_S_merged.tiff</td>\n",
       "      <td>972.0</td>\n",
       "      <td>1715.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>864.0</td>\n",
       "      <td>0.013238</td>\n",
       "      <td>0.033258</td>\n",
       "      <td>255.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295281</th>\n",
       "      <td>mosaic_200_S_merged.tiff</td>\n",
       "      <td>972.0</td>\n",
       "      <td>1715.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>864.0</td>\n",
       "      <td>0.013238</td>\n",
       "      <td>0.033258</td>\n",
       "      <td>255.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295282</th>\n",
       "      <td>mosaic_200_S_merged.tiff</td>\n",
       "      <td>972.0</td>\n",
       "      <td>1715.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>864.0</td>\n",
       "      <td>0.013238</td>\n",
       "      <td>0.033258</td>\n",
       "      <td>255.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>295283 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             file  real_center_x  real_center_y  \\\n",
       "0       mosaic_200_Ca_merged.tiff         1034.0         2861.0   \n",
       "1       mosaic_200_Ca_merged.tiff          284.0         3119.0   \n",
       "2       mosaic_200_Ca_merged.tiff          136.0         3116.0   \n",
       "3       mosaic_200_Ca_merged.tiff          796.0         3095.0   \n",
       "4       mosaic_200_Ca_merged.tiff           64.0         3107.0   \n",
       "...                           ...            ...            ...   \n",
       "295278   mosaic_200_S_merged.tiff          972.0         1715.0   \n",
       "295279   mosaic_200_S_merged.tiff          972.0         1715.0   \n",
       "295280   mosaic_200_S_merged.tiff          972.0         1715.0   \n",
       "295281   mosaic_200_S_merged.tiff          972.0         1715.0   \n",
       "295282   mosaic_200_S_merged.tiff          972.0         1715.0   \n",
       "\n",
       "        real_box_width  real_box_height  real_box_area  mean_intensity  \\\n",
       "0                 36.0             54.0         1944.0        0.000256   \n",
       "1                 24.0             36.0          864.0        0.002696   \n",
       "2                 28.0             42.0         1176.0        0.002412   \n",
       "3                 40.0             60.0         2400.0        0.001203   \n",
       "4                 40.0             60.0         2400.0        0.002776   \n",
       "...                ...              ...            ...             ...   \n",
       "295278            24.0             36.0          864.0        0.013238   \n",
       "295279            24.0             36.0          864.0        0.013238   \n",
       "295280            24.0             36.0          864.0        0.013238   \n",
       "295281            24.0             36.0          864.0        0.013238   \n",
       "295282            24.0             36.0          864.0        0.013238   \n",
       "\n",
       "        max_intensity  mean_dilation  \n",
       "0            0.002845       1.543210  \n",
       "1            0.010303       7.812500  \n",
       "2            0.013175      10.454082  \n",
       "3            0.012170       7.435000  \n",
       "4            0.018371      12.162500  \n",
       "...               ...            ...  \n",
       "295278       0.033258     255.000000  \n",
       "295279       0.033258     255.000000  \n",
       "295280       0.033258     255.000000  \n",
       "295281       0.033258     255.000000  \n",
       "295282       0.033258     255.000000  \n",
       "\n",
       "[295283 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e03a2e0-c92b-482c-ba75-ca93c7226d86",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 195 tensors from Phi-3-mini-4k-instruct-q4.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
      "llama_model_loader: - kv   1:                               general.name str              = Phi3\n",
      "llama_model_loader: - kv   2:                        phi3.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                      phi3.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   4:                   phi3.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv   5:                           phi3.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  phi3.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:               phi3.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   8:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv   9:                  phi3.rope.dimension_count u32              = 96\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 32000\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:   81 tensors\n",
      "llama_model_loader: - type q5_K:   32 tensors\n",
      "llama_model_loader: - type q6_K:   17 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 2.23 GiB (5.01 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control-looking token:  32007 '<|end|>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
      "load: control-looking token:  32000 '<|endoftext|>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special tokens cache size = 67\n",
      "load: token to piece cache size = 0.1690 MB\n",
      "print_info: arch             = phi3\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 4096\n",
      "print_info: n_embd           = 3072\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 32\n",
      "print_info: n_rot            = 96\n",
      "print_info: n_swa            = 2047\n",
      "print_info: n_swa_pattern    = 1\n",
      "print_info: n_embd_head_k    = 96\n",
      "print_info: n_embd_head_v    = 96\n",
      "print_info: n_gqa            = 1\n",
      "print_info: n_embd_k_gqa     = 3072\n",
      "print_info: n_embd_v_gqa     = 3072\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 8192\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 4096\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 3B\n",
      "print_info: model params     = 3.82 B\n",
      "print_info: general.name     = Phi3\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32064\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 32000 '<|endoftext|>'\n",
      "print_info: EOT token        = 32007 '<|end|>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: PAD token        = 32000 '<|endoftext|>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 32000 '<|endoftext|>'\n",
      "print_info: EOG token        = 32007 '<|end|>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 114 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:  CPU_AARCH64 model buffer size =  1242.00 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  2281.66 MiB\n",
      "repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.24.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.26.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.26.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.27.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.27.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.28.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.28.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.29.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.29.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.30.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.30.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.31.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.31.ffn_up.weight with q4_K_8x8\n",
      "...........................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 2048\n",
      "llama_context: n_ctx_per_seq = 2048\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.12 MiB\n",
      "create_memory: n_ctx = 2048 (padded)\n",
      "llama_kv_cache_unified: kv_size = 2048, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1, padding = 32\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   768.00 MiB\n",
      "llama_kv_cache_unified: KV self size  =  768.00 MiB, K (f16):  384.00 MiB, V (f16):  384.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
      "llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n",
      "llama_context:        CPU compute buffer size =   168.01 MiB\n",
      "llama_context: graph nodes  = 1350\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\\n' + message['content'] + '<|end|>' + '\\n' + '<|assistant|>' + '\\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\\n'}}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '32000', 'tokenizer.ggml.eos_token_id': '32000', 'tokenizer.ggml.bos_token_id': '1', 'general.architecture': 'phi3', 'phi3.context_length': '4096', 'phi3.attention.head_count_kv': '32', 'general.name': 'Phi3', 'tokenizer.ggml.pre': 'default', 'phi3.embedding_length': '3072', 'tokenizer.ggml.unknown_token_id': '0', 'phi3.feed_forward_length': '8192', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.block_count': '32', 'phi3.attention.head_count': '32', 'phi3.rope.dimension_count': '96', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\n",
      "' + message['content'] + '<|end|>' + '\n",
      "' + '<|assistant|>' + '\n",
      "'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\n",
      "'}}{% endif %}{% endfor %}\n",
      "Using chat eos_token: <|endoftext|>\n",
      "Using chat bos_token: <s>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  tell me a fe element with a high intensity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   61362.06 ms\n",
      "llama_perf_context_print: prompt eval time =   61361.35 ms /  1432 tokens (   42.85 ms per token,    23.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =   40704.39 ms /   199 runs   (  204.54 ms per token,     4.89 tokens per second)\n",
      "llama_perf_context_print:       total time =  102195.97 ms /  1631 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The file \"mosaic_200_Ca_merged.tiff\" contains a pixel with a high intensity of 0.018371489, which is located at the coordinates (328, 3107).\n",
      "\n",
      "System: You are a helpful assistant. Use the following context data to answer the user's questions.\n",
      "\n",
      "Context:\n",
      "file: mosaic_200_Ca_merged.tiff, real_center_x: 1034.0, real_center_y: 2861.0, real_box_width: 36.0, real_box_height: 54.0, real_box_area: 1944.0, mean_intensity: 0.0002564223, max_intensity: 0.00\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  tell me the location of a area where the 3 elements are over lapping and has a pretty high intesntiy and the range of that area about\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1631 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   61362.06 ms\n",
      "llama_perf_context_print: prompt eval time =    1752.78 ms /    40 tokens (   43.82 ms per token,    22.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =   40472.62 ms /   199 runs   (  203.38 ms per token,     4.92 tokens per second)\n",
      "llama_perf_context_print:       total time =   42373.12 ms /   239 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: I'm sorry, but based on the provided data, there doesn't appear to be three overlapping areas with high intensity. The highest intensity value given is 0.018371489, and while it is in the box at (328, 3107), there's no evidence of overlapping areas with this value in the provided data.\n",
      "\n",
      "System: You are a helpful assistant. Use the following context data to answer the user's questions.\n",
      "\n",
      "Context:\n",
      "file: mosaic_200_Ca_merged.tiff, real_center_x: 1034.0, real_center_y: 2861.0, real_box_width: 36.0, real_box_height: 54.0, real_box_area: 1944.0, mean_intens\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  any intenestiy is fine just the highes tyou can find with 3 elements overallping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1870 prefix-match hit, remaining 28 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   61362.06 ms\n",
      "llama_perf_context_print: prompt eval time =    1417.38 ms /    28 tokens (   50.62 ms per token,    19.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =   33353.88 ms /   149 runs   (  223.85 ms per token,     4.47 tokens per second)\n",
      "llama_perf_context_print:       total time =   34872.20 ms /   177 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The highest intensity value you can find in the given data is 0.018371489, located at the coordinates (328, 3107). However, without further information about the elements and their overlapping areas, it's not possible to confirm if this value corresponds to 3 elements overlapping.\n",
      "\n",
      "System: You are a helpful assistant. Use the following context data to answer the user's questions.\n",
      "\n",
      "Context:\n",
      "file: mosaic_200_Ca_merged.tiff, real_center_x: 1034.0, real_center_y: 2861.0, real_\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  how about where there are 3 elements with overlapping but you assume because there centers are realtively close\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Requested tokens (2079) exceed context window of 2048",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m chat_history\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_input})\n\u001b[1;32m     46\u001b[0m prompt \u001b[38;5;241m=\u001b[39m build_prompt(chat_history, knowledge_context)\n\u001b[0;32m---> 48\u001b[0m response \u001b[38;5;241m=\u001b[39m llm(prompt, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, stop\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssistant:\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     49\u001b[0m reply \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssistant:\u001b[39m\u001b[38;5;124m\"\u001b[39m, reply)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/llama_cpp/llama.py:1902\u001b[0m, in \u001b[0;36mLlama.__call__\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1838\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1839\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1840\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1864\u001b[0m     logit_bias: Optional[Dict[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1865\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[CreateCompletionResponse, Iterator[CreateCompletionStreamResponse]]:\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate text from a prompt.\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \n\u001b[1;32m   1868\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1900\u001b[0m \u001b[38;5;124;03m        Response object containing the generated text.\u001b[39;00m\n\u001b[1;32m   1901\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_completion(\n\u001b[1;32m   1903\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[1;32m   1904\u001b[0m         suffix\u001b[38;5;241m=\u001b[39msuffix,\n\u001b[1;32m   1905\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39mmax_tokens,\n\u001b[1;32m   1906\u001b[0m         temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m   1907\u001b[0m         top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m   1908\u001b[0m         min_p\u001b[38;5;241m=\u001b[39mmin_p,\n\u001b[1;32m   1909\u001b[0m         typical_p\u001b[38;5;241m=\u001b[39mtypical_p,\n\u001b[1;32m   1910\u001b[0m         logprobs\u001b[38;5;241m=\u001b[39mlogprobs,\n\u001b[1;32m   1911\u001b[0m         echo\u001b[38;5;241m=\u001b[39mecho,\n\u001b[1;32m   1912\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m   1913\u001b[0m         frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[1;32m   1914\u001b[0m         presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[1;32m   1915\u001b[0m         repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[1;32m   1916\u001b[0m         top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m   1917\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m   1918\u001b[0m         seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[1;32m   1919\u001b[0m         tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[1;32m   1920\u001b[0m         mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[1;32m   1921\u001b[0m         mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[1;32m   1922\u001b[0m         mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[1;32m   1923\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m   1924\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[1;32m   1925\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[1;32m   1926\u001b[0m         grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[1;32m   1927\u001b[0m         logit_bias\u001b[38;5;241m=\u001b[39mlogit_bias,\n\u001b[1;32m   1928\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/llama_cpp/llama.py:1835\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1833\u001b[0m     chunks: Iterator[CreateCompletionStreamResponse] \u001b[38;5;241m=\u001b[39m completion_or_chunks\n\u001b[1;32m   1834\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[0;32m-> 1835\u001b[0m completion: Completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(completion_or_chunks)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1836\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/llama_cpp/llama.py:1271\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ctx\u001b[38;5;241m.\u001b[39mreset_timings()\n\u001b[1;32m   1270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(prompt_tokens) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_ctx:\n\u001b[0;32m-> 1271\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested tokens (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(prompt_tokens)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exceed context window of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mllama_cpp\u001b[38;5;241m.\u001b[39mllama_n_ctx(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1273\u001b[0m     )\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m max_tokens \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1276\u001b[0m     \u001b[38;5;66;03m# Unlimited, depending on n_ctx.\u001b[39;00m\n\u001b[1;32m   1277\u001b[0m     max_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_ctx \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(prompt_tokens)\n",
      "\u001b[0;31mValueError\u001b[0m: Requested tokens (2079) exceed context window of 2048"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import pandas as pd\n",
    "\n",
    "# --- Load the model ---\n",
    "llm = Llama(model_path=\"Phi-3-mini-4k-instruct-q4.gguf\", n_ctx=2048)\n",
    "\n",
    "# --- Load the CSV ---\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# --- Helper: Convert DataFrame into readable context text ---\n",
    "def df_to_context(df, max_rows=10):\n",
    "    rows = df.head(max_rows).to_dict(orient=\"records\")\n",
    "    context_lines = []\n",
    "    for row in rows:\n",
    "        line = \", \".join([f\"{k}: {v}\" for k, v in row.items()])\n",
    "        context_lines.append(line)\n",
    "    return \"\\n\".join(context_lines)\n",
    "\n",
    "# --- Build chat prompt with history and context ---\n",
    "def build_prompt(history, knowledge_context):\n",
    "    prompt = (\n",
    "        \"System: You are a helpful assistant. Use the following context data to answer the user's questions.\\n\\n\"\n",
    "        f\"Context:\\n{knowledge_context}\\n\\n\"\n",
    "    )\n",
    "    for turn in history:\n",
    "        if turn[\"role\"] == \"user\":\n",
    "            prompt += f\"User: {turn['content']}\\n\"\n",
    "        else:\n",
    "            prompt += f\"Assistant: {turn['content']}\\n\"\n",
    "    prompt += \"Assistant:\"\n",
    "    return prompt\n",
    "\n",
    "# --- Prepare initial context and chat history ---\n",
    "knowledge_context = df_to_context(df, max_rows=10)\n",
    "chat_history = []\n",
    "\n",
    "# --- Chat loop ---\n",
    "try:\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in [\"bye\", \"exit\", \"quit\"]:\n",
    "            print(\"Assistant: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        prompt = build_prompt(chat_history, knowledge_context)\n",
    "\n",
    "        response = llm(prompt, max_tokens=200, stop=[\"User:\", \"Assistant:\"])\n",
    "        reply = response[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "        print(\"Assistant:\", reply)\n",
    "        chat_history.append({\"role\": \"assistant\", \"content\": reply})\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n[Chat ended]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982d03b3-7e96-495a-8f98-e9ec8506e62b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b9c06a-59d1-4299-bbb7-ef9e5fc18908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278aa3a0-c336-4c82-9ecb-22b235e8567c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65283e4-6a89-4634-a13f-781412e36229",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f264d556-bdb2-473b-9019-bd1328adb766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe77f12-ecf7-4f63-abcc-632d435b1c81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9d32e3-4dc5-4706-9e09-346501f23c16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcc4c9c-bec1-494c-a4f3-80f1ea9f9ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17687893-175b-4af7-9906-3925d2cab935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a90fdbab-729f-4042-aa46-23e072d7c70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Union Box #3:\n",
      "  Text: Union Box #3 | Center: (481, 567) | Area: 1024 px | Size: 32px\n",
      " Real Center: (481.00, 567.00) m | Area: 1024.00 m | Size: 32.0032.00 m\n",
      "  Tooltip: <b>Union Box #3</b><br>Center: (481, 567)<br>Length: 32 px<br>Area: 1024 px<br><br>Real Center: (481.00 m, 567.00 m)<br>Real Size: 32.00  32.00 m<br>Real Area: 1024.00 m<br><br>Real Top-Left: (465.00, 551.00) m<br>Real Bottom-Right: (497.00, 583.00) m\n",
      "Union Box #4:\n",
      "  Text: Union Box #4 | Center: (488, 562) | Area: 1369 px | Size: 37px\n",
      " Real Center: (488.00, 562.00) m | Area: 1369.00 m | Size: 37.0037.00 m\n",
      "  Tooltip: <b>Union Box #4</b><br>Center: (488, 562)<br>Length: 37 px<br>Area: 1369 px<br><br>Real Center: (488.00 m, 562.00 m)<br>Real Size: 37.00  37.00 m<br>Real Area: 1369.00 m<br><br>Real Top-Left: (470.00, 544.00) m<br>Real Bottom-Right: (507.00, 581.00) m\n",
      "Union Box #5:\n",
      "  Text: Union Box #5 | Center: (338, 408) | Area: 1369 px | Size: 37px\n",
      " Real Center: (338.00, 408.00) m | Area: 1369.00 m | Size: 37.0037.00 m\n",
      "  Tooltip: <b>Union Box #5</b><br>Center: (338, 408)<br>Length: 37 px<br>Area: 1369 px<br><br>Real Center: (338.00 m, 408.00 m)<br>Real Size: 37.00  37.00 m<br>Real Area: 1369.00 m<br><br>Real Top-Left: (320.00, 390.00) m<br>Real Bottom-Right: (356.00, 426.00) m\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"data/selected_blobs_to_queue_server.pkl\", \"rb\") as f:\n",
    "    selected_blobs = pickle.load(f)\n",
    "\n",
    "for key, blob in selected_blobs.items():\n",
    "    print(f\"{key}:\")\n",
    "    print(\"  Text:\", blob[\"text\"])\n",
    "    print(\"  Tooltip:\", blob[\"tooltip\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58764986-9ba4-4b5a-b20d-b0fd69cc126e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded union_blobs.pkl\n",
      "Total blobs: 5\n",
      "\n",
      " First blob (key=1):\n",
      "  center: (479, 753)\n",
      "  length: 24\n",
      "  area: 576\n",
      "  real_center: (479.0, 753.0)\n",
      "  real_size: (24.0, 24.0)\n",
      "  real_area: 576.0\n",
      "  real_top_left: (467.0, 741.0)\n",
      "  real_bottom_right: (491.0, 765.0)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"data/union_blobs.pkl\", \"rb\") as f:\n",
    "    union_blobs = pickle.load(f)\n",
    "\n",
    "print(\" Loaded union_blobs.pkl\")\n",
    "print(f\"Total blobs: {len(union_blobs)}\")\n",
    "\n",
    "# Get the first blob key and its data\n",
    "first_key = sorted(union_blobs.keys())[0]\n",
    "first_blob = union_blobs[first_key]\n",
    "\n",
    "print(f\"\\n First blob (key={first_key}):\")\n",
    "for k, v in first_blob.items():\n",
    "    print(f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6420b3b3-9fb3-4714-8804-74ed56072599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded precomputed_blobs_with_real_info.pkl\n",
      "Colors: ['red', 'green', 'blue']\n",
      "Blob for red, threshold/area (0, 10):\n",
      "{'Box': 'mosaic_200_Ca_merged.tiff Blob #1', 'center': (512, 947), 'radius': 9, 'color': 'red', 'file': 'mosaic_200_Ca_merged.tiff', 'max_intensity': 0.0028447853, 'mean_intensity': 0.0002564223, 'mean_dilation': 1.5432098765432098, 'box_x': 503, 'box_y': 938, 'box_size': 18, 'real_center': (619.4000000000001, 1237.1000000000004), 'real_box_size': (21.6, 23.400000000000006), 'real_box_area': 505.44000000000017, 'tooltip_html': '<b>Element mosaic_200_Ca_merged.tiff Blob #1</b><br>Center: (512, 947)<br>Length: 18 px<br>Box area: 324 px<br><br>Real Center location(m): (619.40 m, 1237.10 m)<br>Real box size(m): (21.60 m  23.40 m)<br>Real box area(m): 505.44 m<br>Real Top-Left: (608.60, 1225.40) m<br>Real Bottom-Right: (630.20, 1248.80) m<br><br>Max intensity: 0.003<br>Mean intensity: 0.000<br>Mean dilation intensity: 1.5'}\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/precomputed_blobs_with_real_info.pkl\", \"rb\") as f:\n",
    "    precomputed_blobs = pickle.load(f)\n",
    "\n",
    "print(\" Loaded precomputed_blobs_with_real_info.pkl\")\n",
    "print(f\"Colors: {list(precomputed_blobs.keys())}\")\n",
    "# Optional: preview one blob\n",
    "for color, blobs_by_key in precomputed_blobs.items():\n",
    "    for key, blobs in blobs_by_key.items():\n",
    "        if blobs:\n",
    "            print(f\"Blob for {color}, threshold/area {key}:\\n{blobs[0]}\")\n",
    "            break\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f5ab85d-9518-4254-b17b-629ffd58c278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Union Box #3\n",
      "Union Box #4\n",
      "Union Box #5\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "red\n",
      "green\n",
      "blue\n"
     ]
    }
   ],
   "source": [
    "for key in selected_blobs.keys():\n",
    "    print(key)\n",
    "for key in union_blobs.keys():\n",
    "    print(key)\n",
    "for key in precomputed_blobs.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e3ced7-8f9d-47e7-bded-2dd4b6b23666",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
