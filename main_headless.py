import json
import os
import pathlib
import traceback as trackback

import numpy as np
import tifffile as tiff

from app_state import AppState
from utils import (
    detect_blobs,
    find_union_blobs,
    headless_send_queue_coarse_scan,
    headless_send_queue_fine_scan,
    normalize_and_dilate,
    save_each_blob_as_individual_scan,
    wait_for_element_tiffs,
)


def load_json_file(path):
    """Loads a single JSON file."""
    if not path.exists():
        raise FileNotFoundError(f"File not found: {path.name}")
    with open(path, "r") as f:
        return json.load(f)


def load_parameters(watch_dir):
    """Loads all required JSON parameter files."""
    print("Looking for analysis_params.json, beamline_params.json, and scan_200_params.json")
    try:
        analysis_params = load_json_file(watch_dir / "analysis_params.json")
        beamline_params = load_json_file(watch_dir / "beamline_params.json")
        scan_params = load_json_file(watch_dir / "scan_200_params.json")
        print("Loaded analysis_params.json:", analysis_params)
        print("Loaded beamline_params.json:", beamline_params)
        print("Loaded scan_200_params.json:", scan_params)
        return analysis_params, beamline_params, scan_params
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"Error reading JSON files: {e}")
        exit(1)


def run_headless_processing():
    """Main function to run the headless processing workflow."""
    state = AppState()

    notebook_dir = pathlib.Path().resolve()
    watch_dir = notebook_dir / "data" / "input"
    headless_dir = notebook_dir / "data" / "headless_scan"
    watch_dir.mkdir(exist_ok=True)
    headless_dir.mkdir(exist_ok=True)

    analysis_params, beamline_params, scan_params = load_parameters(watch_dir)

    # Populate state from loaded params
    state.microns_per_pixel_x = scan_params.get("microns_per_pixel_x")
    state.microns_per_pixel_y = scan_params.get("microns_per_pixel_y")
    state.true_origin_x = scan_params.get("true_origin_x")
    state.true_origin_y = scan_params.get("true_origin_y")
    state.element_colors = analysis_params.get("element_list", [])

    # Perform coarse scan
    print("\nStarting coarse scan...")
    initial_scan_path = watch_dir / "initial_scan.json"
    headless_send_queue_coarse_scan(str(headless_dir), beamline_params, initial_scan_path)

    # Wait for TIFF files to be generated by the coarse scan
    '''
    tiff_paths = wait_for_element_tiffs(state.element_colors, headless_dir)

    # Define up to 8 colors for blob detection
    COLOR_ORDER = [
        'red', 'blue', 'green', 'orange', 'purple', 'cyan', 'olive', 'yellow'
    ]

    print("\nProcessing TIFF files for blob detection...")
    state.precomputed_blobs = {color: {} for color in COLOR_ORDER}

    max_tiffs = min(len(tiff_paths), 8)
    processed_elements = list(tiff_paths.keys())[:max_tiffs]

    for idx, element in enumerate(processed_elements):
        color = COLOR_ORDER[idx]
        tiff_path = tiff_paths[element]
        print(f"Processing {tiff_path.name} as color {color}")
        try:
            tiff_img = tiff.imread(str(tiff_path)).astype(np.float32)
            tiff_norm, tiff_dilated = normalize_and_dilate(tiff_img)
            blobs = detect_blobs(
                tiff_dilated,
                tiff_norm,
                analysis_params["min_threshold_intensity"],
                analysis_params["min_threshold_area"],
                color,
                tiff_path.name
            )
            state.precomputed_blobs[color][(analysis_params["min_threshold_intensity"], analysis_params["min_threshold_area"])] = blobs
        except Exception as e:
            print(f"❌ Error processing {tiff_path.name}: {e}")
            trackback.print_exc()

    if len(processed_elements) < 2:
        print("Not enough TIFFs to perform union operation (need at least 2 Elements)")
        return

    print("\nFinding union blobs...")
    unions = find_union_blobs(
        state.precomputed_blobs,
        state.microns_per_pixel_x,
        state.microns_per_pixel_y,
        state.true_origin_x,
        state.true_origin_y
    )

    formatted_unions = {}
    print("Processing and formatting union data...")

    for idx, union in unions.items():
        box_name = f"Union Box #{idx}"
        formatted = {
            "text": box_name,
            "image_center": union["center"],
            "image_length": union["length"],
            "image_area_px²": union["area"],
            "real_center_um": union["real_center_um"],
            "real_size_um": union["real_size_um"],
            "real_area_um²": union["real_area_um²"],
            "real_top_left_um": union["real_top_left_um"],
            "real_bottom_right_um": union["real_bottom_right_um"]
        }
        formatted_unions[box_name] = formatted

    # Save union data to file
    output_path = headless_dir / "unions_output.json"
    with open(output_path, "w") as f:
        json.dump(formatted_unions, f, indent=2)
    print(f"\n✅ Union data saved to: {output_path}")

    # Save individual scans for fine scanning
    save_each_blob_as_individual_scan(formatted_unions, output_dir=str(headless_dir))

    # Send fine scans to the queue
    headless_send_queue_fine_scan(str(headless_dir), beamline_params, scan_ID=None) # Assuming scan_ID is not needed here
    '''

if __name__ == "__main__":
    run_headless_processing()